{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "cf6729d67d579994890b66e39dd4ca4905be91a0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# some linear models\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "\n",
    "# SCM for classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to download data\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4377b5e5fe86395ca9882bbc0965f6960a8d21fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7e54cc9a009569af9de9e490acec589d933a9147"
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv')\n",
    "df_test=pd.read_csv('test.csv')\n",
    "df_cat=pd.read_json('categories.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Group']=df_train.image_path.map(lambda x: x[:7])\n",
    "df_test['Group']=df_test.image_path.map(lambda x: x[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train,test=train_test_split(df_train,random_state=2019,stratify=df_train.Category,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e1fb7371a737c677d4e038752b4296e4044048f5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "btb new blus casual wanita tanpa lengan dengan bahan sifon dan potongan longgar bergaya sexy\n",
      "iphone 7 plus 128gb red fullset original\n",
      "sale ben nye neutral set colorless powder poudre incolore 25 g\n",
      "terlaris dermovate cream hijau original\n",
      "termurah mini dress brokat gaun pesta murah party wanita\n",
      "samasung s7 flat\n",
      "obral 2018 new cotton checkered plaid blouses shirt cage female long sleeve casual slim women\n",
      "marcks bedak tabur 20 g new cn86\n",
      "new loreal paris true match blur cream 25ml stok terbatas\n",
      "sweater kaos hoodie lengan panjang bahan velvet untuk pria wanita\n"
     ]
    }
   ],
   "source": [
    "# Let's check few examples\n",
    "for i in range(10):\n",
    "    print(train.title.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a7733605ba55f99eed740a059321a159e990999"
   },
   "source": [
    "## Building seperate models for each Groupb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c15a893b7cf3ab107a65cd3536df387685611664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n"
     ]
    }
   ],
   "source": [
    "ignore_words = set(stopwords.words('english')).union(set(stopwords.words('indonesian')))\n",
    "print(len(ignore_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "8d6d5685a62a969e76fd1dfa38796b527a551de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words are reduced by introducing stop words 70586 70968\n"
     ]
    }
   ],
   "source": [
    "# let's check for all of the data. This slightly helps to reduce dimensionality\n",
    "cv=CountVectorizer(stop_words=ignore_words)\n",
    "cv1=CountVectorizer() # No, stop words\n",
    "X_t=cv.fit_transform(train['title'])\n",
    "X_t1=cv1.fit_transform(train['title'])\n",
    "print('Some words are reduced by introducing stop words',X_t.shape[1],X_t1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "fa40b1e0343a64c28bf7a32ab348e42eb3c074a1"
   },
   "outputs": [],
   "source": [
    "# let's build pipeline for parameter search in n_grams and and various models\n",
    "lr=LogisticRegression()\n",
    "nb=MultinomialNB()\n",
    "svc=SVC()\n",
    "# let's check first three and see which one is best\n",
    "gnb=GaussianNB()\n",
    "bnb=BernoulliNB()\n",
    "br=BayesianRidge()\n",
    "\n",
    "pipe_lr=Pipeline([('vectorize', CountVectorizer(stop_words =ignore_words)),('model',lr)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "30661f1b6c26ca2ebf30249cbf084bcc1a946b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words={'jang...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe_lr.fit(train[train.Group=='beauty_'].title,train[train.Group=='beauty_'].Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "74ee95d37eb94c9b6b538bffbf1b665caa261093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred=pipe_lr.predict(test[test.Group=='beauty_'].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d1e1a0c2aa128fe330f99dcdda06d3c2f2aa2440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7626009735331577"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "accuracy_score(test[test.Group=='beauty_'].Category,pred)\n",
    "\n",
    "\n",
    "# without removing stop words :0.76361288971858265\n",
    "# removing stop_words: 0.7626009735331577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1c972c1e9ca335f643ba32ca27eab9641a95ab0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 446 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score=pipe_lr.score(test[test.Group=='beauty_'].title,test[test.Group=='beauty_'].Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "cb6c68515e239a6e0c30df3c9e87b580f61b8346"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously I have tried both TfidfVectorizer, CountVectorizer for both naive bayes and logistic regression. But it turns out that Countvectorizer with logistic regression gives best results. I have removed stop words from english and indonesian. Than, I did parameter optimization for n_grams. It turns out that best ngram_range is (1,3 for beauty and mobile products and (1,5) for fashion. \n",
    "\n",
    "## (1,5) might be not the most optimal we might still imporove by increasing it.\n",
    "\n",
    "\n",
    "## I also run this simulation for NB. The optimal ngram_range was ... I lost it in kernel!!\n",
    "\n",
    "## but it was smth like for mobile (1,2) and for beauty and fashion (2,5) we can do it again, it won't take much time. Score were still less than logistic regression\n",
    "\n",
    "## I also tried svc (SVM) but it takes to much time imposimble to train espacially if there is many features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict with naive bayes approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nb_beauty=Pipeline([('vectorize', CountVectorizer(stop_words =ignore_words,ngram_range=(2,6))),('model',nb)])\n",
    "pipe_nb_mobile=Pipeline([('vectorize', CountVectorizer(stop_words =ignore_words,ngram_range=(1,2))),('model',nb)])\n",
    "pipe_nb_fashion=Pipeline([('vectorize', CountVectorizer(stop_words =ignore_words,ngram_range=(2,6))),('model',nb)])\n",
    "\n",
    "pipe_nb_all=[pipe_nb_beauty,pipe_nb_mobile,pipe_nb_fashion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauty_', 'mobile_', 'fashion']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get predictions and predict proba for naive bayes approch\n",
    "pred_nb_proba_val={}\n",
    "pred_nb_proba_test={}\n",
    "classes={}\n",
    "classes_all={}\n",
    "for i in range(3):\n",
    "    model=pipe_nb_all[i]\n",
    "    X=train[train.Group==groups[i]].title\n",
    "    y=train[train.Group==groups[i]].Category\n",
    "    model.fit(X,y)\n",
    "    classes[groups[i]]=model.classes_\n",
    "    pred_nb_proba_val[groups[i]]=model.predict_proba(test[test.Group==groups[i]].title)\n",
    "    # Train to all data\n",
    "    model=pipe_nb_all[i]\n",
    "    X=df_train[df_train.Group==groups[i]].title\n",
    "    y=df_train[df_train.Group==groups[i]].Category\n",
    "    model.fit(X,y)\n",
    "    classes_all[groups[i]]=model.classes_\n",
    "    pred_nb_proba_test[groups[i]]=model.predict_proba(df_test[df_test.Group==groups[i]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving probabilities of validation set\n",
    "for i in range(3):\n",
    "    df=pd.DataFrame(pred_nb_proba_val[groups[i]])\n",
    "    df.columns=classes[groups[i]]\n",
    "    df['pred']=df.idxmax(axis=1) # do it before placing itemid or category otherwise column type will become string type\n",
    "    df['itemid']=test[test.Group==groups[i]].itemid.values # values is essensial since indexes are different\n",
    "    df['Category']=test[test.Group==groups[i]].Category.values\n",
    "    df.to_csv(groups[i]+'_val_proba_nlp_nb.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Predictions for final test set\n",
    "for i in range(3):\n",
    "    df=pd.DataFrame(pred_nb_proba_test[groups[i]])\n",
    "    df.columns=classes[groups[i]]\n",
    "    df['pred']=df.idxmax(axis=1)\n",
    "    df['itemid']=df_test[df_test.Group==groups[i]].itemid.values\n",
    "    df.to_csv(groups[i]+'_test_proba_nlp_nb.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for the whole model and save test predict proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now use all data and save predict proba of model with all data\n",
    "pred_nb_proba_test_all={}\n",
    "classes={}\n",
    "for i in range(3):\n",
    "    model=pipe_nb_all[i]\n",
    "    X=df_train[df_train.Group==groups[i]].title\n",
    "    y=df_train[df_train.Group==groups[i]].Category\n",
    "    model.fit(X,y)\n",
    "    classes[groups[i]]=model.classes_\n",
    "    pred_nb_proba_test_all[groups[i]]=model.predict_proba(df_test[df_test.Group==groups[i]].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Predictions for final test set\n",
    "for i in range(3):\n",
    "    df=pd.DataFrame(pred_nb_proba_test_all[groups[i]])\n",
    "    df.columns=classes[groups[i]]\n",
    "    df['pred']=df.idxmax(axis=1)\n",
    "    df['itemid']=df_test[df_test.Group==groups[i]].itemid.values\n",
    "    df.to_csv(groups[i]+'_test_proba_nlp_nb_all.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
